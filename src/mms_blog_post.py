"""Fine_Tune_MMS_on_Common_Voice.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_MMS_on_Common_Voice.ipynb
"""

import concurrent.futures
import json
import os
import random
import re
import tarfile
import tempfile
import threading
from collections.abc import Iterator
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Protocol, runtime_checkable

import boto3
import comet_ml
import numpy as np
import polars as pl
import torch
import torchaudio as ta
import uroman
from botocore.exceptions import ClientError
from datasets import Audio, load_dataset
from datasets import Dataset as HFDataset
from evaluate import load
from safetensors.torch import save_file as safe_save_file
from torch import Generator, nn
from torch.utils.data import Dataset as TorchDataset
from torch.utils.data import RandomSampler, SequentialSampler
from tqdm.auto import tqdm
from transformers import (
    EvalPrediction,
    Trainer,
    TrainingArguments,
    Wav2Vec2Config,
    Wav2Vec2CTCTokenizer,
    Wav2Vec2FeatureExtractor,
    Wav2Vec2ForCTC,
    Wav2Vec2Model,
    Wav2Vec2Processor,
    set_seed,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.models.wav2vec2.modeling_wav2vec2 import (
    _HIDDEN_STATES_START_POSITION,
    WAV2VEC2_ADAPTER_SAFE_FILE,
    CausalLMOutput,
    Wav2Vec2BaseModelOutput,
)
from transformers.trainer_pt_utils import LengthGroupedSampler
from transformers.trainer_utils import has_length
from transformers.utils import is_flash_attn_2_available
from types_boto3_s3.client import S3Client

import wandb


@dataclass
class DataCollatorCTCWithPadding:
    """
    Data collator that will dynamically pad the inputs received.

    Args:
        processor (:class:`~transformers.Wav2Vec2Processor`)
            The processor used for processing the data.
        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:
            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
              sequence if provided).
            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
              maximum acceptable input length for the model if that argument is not provided.
            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
              different lengths).
    """

    processor: Wav2Vec2Processor
    padding: bool | str = True

    def __call__(
        self, features: list[dict[str, list[int] | torch.Tensor]]
    ) -> dict[str, torch.Tensor]:
        """Collates the features."""
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [
            {"input_values": feature["input_values"]} for feature in features
        ]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            return_tensors="pt",
        )
        labels_batch = self.processor.pad(
            labels=label_features,
            padding=self.padding,
            return_tensors="pt",
        )

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        batch["labels"] = labels
        batch["flat_labels"] = torch.cat(
            [torch.tensor(feature["labels"]) for feature in features]
        )  # type: ignore

        return batch


def _compute_mask_indices_with_lengths(
    shape: tuple[int, int],
    mask_prob: float,
    mask_length: int,
    output_lengths: torch.LongTensor | None = None,
    min_masks: int = 0,
) -> np.ndarray:
    """Spec augment indices.

    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for
    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on
    CPU as part of the preprocessing during training.

    Args:
        shape: The shape for which to compute masks. This should be of a tuple of size 2 where
               the first element is the batch size and the second element is the length of the axis to span.
        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of
                    independently generated mask spans of length `mask_length` is computed by
                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the
                    actual percentage will be smaller.
        mask_length: size of the mask
        min_masks: minimum number of masked spans
        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of
                        each batch dimension.
    """
    batch_size, sequence_length = shape

    if mask_length < 1:
        raise ValueError("`mask_length` has to be bigger than 0.")

    if mask_length > sequence_length:
        raise ValueError(
            f"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}"
            f" and `sequence_length`: {sequence_length}`"
        )

    # epsilon is used for probabilistic rounding
    epsilon = np.random.rand(1).item()

    def compute_num_masked_span(input_length: int) -> int:
        """Given input length, compute how many spans should be masked."""
        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)
        num_masked_span = max(num_masked_span, min_masks)

        # make sure num masked span <= sequence_length
        if num_masked_span * mask_length > sequence_length:
            num_masked_span = sequence_length // mask_length

        # make sure num_masked span is also <= input_length - (mask_length - 1)
        if input_length - (mask_length - 1) < num_masked_span:
            num_masked_span = max(input_length - (mask_length - 1), 0)

        return num_masked_span

    # compute number of masked spans in batch
    input_lengths = (
        output_lengths.detach().tolist()
        if output_lengths is not None
        else [sequence_length for _ in range(batch_size)]
    )

    # SpecAugment mask to fill
    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)
    spec_aug_mask_idxs = []

    max_num_masked_span = compute_num_masked_span(sequence_length)

    if max_num_masked_span == 0:
        return spec_aug_mask

    for input_length in input_lengths:
        # compute num of masked spans for this input
        num_masked_span = compute_num_masked_span(input_length)

        # get random indices to mask
        spec_aug_mask_idx = np.random.choice(
            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False
        )

        # pick first sampled index that will serve as a dummy index to pad vector
        # to ensure same dimension for all batches due to probabilistic rounding
        # Picking first sample just pads those vectors twice.
        if len(spec_aug_mask_idx) == 0:
            # this case can only happen if `input_length` is strictly smaller then
            # `sequence_length` in which case the last token has to be a padding
            # token which we can use as a dummy mask id
            dummy_mask_idx = sequence_length - 1
        else:
            dummy_mask_idx = spec_aug_mask_idx[0]

        spec_aug_mask_idx = np.concatenate(
            [
                spec_aug_mask_idx,
                np.ones(max_num_masked_span - num_masked_span, dtype=np.int32)
                * dummy_mask_idx,
            ]
        )
        spec_aug_mask_idxs.append(spec_aug_mask_idx)

    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)

    # expand masked indices to masked spans
    spec_aug_mask_idxs = np.broadcast_to(
        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)
    )
    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(
        batch_size, max_num_masked_span * mask_length
    )

    # add offset to the starting indexes so that indexes now create a span
    offsets = np.arange(mask_length)[None, None, :]
    offsets = np.broadcast_to(
        offsets, (batch_size, max_num_masked_span, mask_length)
    ).reshape(batch_size, max_num_masked_span * mask_length)
    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets

    # ensure that we cannot have indices larger than sequence_length
    if spec_aug_mask_idxs.max() > sequence_length - 1:
        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = (
            sequence_length - 1
        )

    # scatter indices to mask
    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)

    return spec_aug_mask


class CustomWav2Vec2ForCTC(Wav2Vec2ForCTC):
    """Custom faster wav2vec2 implementation."""

    def __init__(self, config: Wav2Vec2Config, target_lang: str | None = None) -> None:
        super().__init__(config)

        # self.wav2vec2 = CustomWav2Vec2Model(config)
        self.wav2vec2 = Wav2Vec2Model(config)
        self.dropout = nn.Dropout(config.final_dropout)

        self.target_lang = target_lang

        if config.vocab_size is None:
            raise ValueError(
                f"You are trying to instantiate {self.__class__} with a configuration that "
                "does not define the vocabulary size of the language model head. Please "
                "instantiate the model as follows: `Wav2Vec2ForCTC.from_pretrained(..., vocab_size=vocab_size)`. "
                "or define `vocab_size` of your model's configuration."
            )
        output_hidden_size = (
            config.output_hidden_size
            if hasattr(config, "add_adapter") and config.add_adapter
            else config.hidden_size
        )
        self.lm_head = nn.Linear(output_hidden_size, config.vocab_size)

        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        input_values: torch.Tensor | None,
        attention_mask: torch.Tensor | None = None,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
        labels: torch.Tensor | None = None,
        flat_labels: torch.Tensor | None = None,
    ) -> tuple | CausalLMOutput:
        """Forward."""
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        if labels is not None and labels.max() >= self.config.vocab_size:
            raise ValueError(
                f"Label values must be <= vocab_size: {self.config.vocab_size}"
            )

        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        hidden_states = self.dropout(hidden_states)

        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None and flat_labels is not None:
            # retrieve loss input_lengths from attention_mask
            attention_mask = (
                attention_mask
                if attention_mask is not None
                else torch.ones_like(input_values, dtype=torch.long)  # type: ignore
            )  # type: ignore
            input_lengths = self._get_feat_extract_output_lengths(
                attention_mask.sum(-1)  # type: ignore
            ).to(torch.long)  # type: ignore

            # assuming that padded tokens are filled with -100
            # when not being attended to
            target_lengths, flattened_targets = self.labels_for_ctc(labels, flat_labels)

            # ctc_loss doesn't support fp16
            log_probs = nn.functional.log_softmax(
                logits, dim=-1, dtype=torch.float32
            ).transpose(0, 1)

            with torch.backends.cudnn.flags(enabled=True, benchmark=True):
                loss = nn.functional.ctc_loss(
                    log_probs,
                    flattened_targets,
                    input_lengths,
                    target_lengths,
                    blank=self.config.pad_token_id,
                    reduction=self.config.ctc_loss_reduction,
                    zero_infinity=self.config.ctc_zero_infinity,
                )

        if not return_dict:
            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]
            return (loss, *output) if loss is not None else output

        return CausalLMOutput(
            loss=loss,  # type: ignore
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def labels_for_ctc(
        self, labels: torch.Tensor, flat_labels: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Return labels formatting for CTC loss."""
        labels_mask = labels >= 0
        target_lengths = labels_mask.sum(-1)
        return target_lengths, flat_labels


def get_length_grouped_indices_shuffled(
    lengths: list[int],
    batch_size: int,
    mega_batch_mult: int | None = None,
    generator: Generator | None = None,
    indices_order: list[int] | None = None,
    grouped_indices: list[list[int]] | None = None,
) -> list[int]:
    """Get shuffled megabatches, A custom version. First is still largest.

    Return a list of indices so that each slice of `batch_size` consecutive indices correspond to elements of similar
    lengths. To do this, the indices are:

    - randomly permuted
    - grouped in mega-batches of size `mega_batch_mult * batch_size`
    - sorted by length in each mega-batch

    The result is the concatenation of all mega-batches, with the batch of `batch_size` containing the element of
    maximum length placed first, so that an OOM happens sooner rather than later.
    """
    # Default for mega_batch_mult: 50 or the number to get 4 megabatches, whichever is smaller.
    if mega_batch_mult is None:
        mega_batch_mult = min(len(lengths) // (batch_size * 4), 50)
        # Just in case, for tiny datasets
        if mega_batch_mult == 0:
            mega_batch_mult = 1

    # We need to use torch for the random part as a distributed sampler will set the random seed for torch.

    if grouped_indices is not None:
        indices = []
        if indices_order is not None:
            grouped_indices = [
                [item for item in group if item in indices_order]
                for group in grouped_indices
            ]
            grouped_indices = [group for group in grouped_indices if (len(group) > 0)]
        for group in grouped_indices:
            group_perm = torch.randperm(len(group), generator=generator)
            group_indices = [group[i] for i in group_perm]
            indices.extend(group_indices)
    elif indices_order is not None:
        indices = torch.randperm(len(indices_order), generator=generator).tolist()
    else:
        indices = torch.randperm(len(lengths), generator=generator).tolist()
    megabatch_size = mega_batch_mult * batch_size
    megabatches = [
        indices[i : i + megabatch_size] for i in range(0, len(indices), megabatch_size)
    ]
    megabatches = [
        sorted(megabatch, key=lambda i: lengths[i], reverse=True)
        for megabatch in megabatches
    ]

    # The rest is to get the biggest batch first.
    # Since each megabatch is sorted by descending length, the longest element is the first
    megabatch_maximums = [lengths[megabatch[0]] for megabatch in megabatches]
    max_idx = torch.argmax(torch.tensor(megabatch_maximums)).item()
    assert isinstance(max_idx, int)
    # Switch to put the longest element in first position
    megabatches[0][0], megabatches[max_idx][0] = (
        megabatches[max_idx][0],
        megabatches[0][0],
    )

    batches = [
        [
            megabatch[i * batch_size : (i + 1) * batch_size]
            for i in range(mega_batch_mult)
            if (i * batch_size < len(megabatch))
        ]
        for megabatch in megabatches
    ]

    first_batch = batches[0][0]
    batches = [batches[0][1:], *batches[1:]]

    batches = [
        [
            megabatch[i]
            for i in torch.randperm(len(megabatch), generator=generator).tolist()
        ]
        for megabatch in batches
    ]
    batches = [[first_batch, *batches[0]], *batches[1:]]

    indices = [i for megabatch in batches for batch in megabatch for i in batch]
    if indices_order is not None:
        assert len(indices) == len(indices_order)
        assert len(set(indices)) == len(indices_order)
        assert max(indices) == len(indices_order) - 1
    else:
        assert len(indices) == len(lengths)
        assert len(set(indices)) == len(lengths)
        assert max(indices) == len(lengths) - 1
    assert min(indices) == 0
    return indices


class CustomLengthGroupedSampler(LengthGroupedSampler):
    """Custom sampler for random order."""

    def __init__(
        self,
        *args: Any,
        mega_batch_mult: int | None = None,
        indices_order: list[int] | None = None,
        grouped_indices: list[list[int]] | None = None,
        **kwargs: Any,
    ) -> None:
        super().__init__(*args, **kwargs)
        self.mega_batch_mult = mega_batch_mult
        self.indices_order = indices_order
        self.grouped_indices = grouped_indices

    def __len__(self) -> int:
        """Get the length of the sampler."""
        if self.indices_order is not None:
            return len(self.indices_order)
        else:
            return len(self.lengths)

    def __iter__(self) -> Iterator[int]:
        """Get iterator with shuffled indices."""
        indices = get_length_grouped_indices_shuffled(
            self.lengths,
            self.batch_size,
            generator=self.generator,
            mega_batch_mult=self.mega_batch_mult,
            indices_order=self.indices_order,
            grouped_indices=self.grouped_indices,
        )
        return iter(indices)


@dataclass
class CustomTrainingArguments(TrainingArguments):
    """Custom training args."""

    mega_batch_mult: int = field(
        default=50, metadata={"help": "The mega batch multiple."}
    )
    has_length_column: bool = field(
        default=True,
        metadata={
            "help": "Sets whether the dataset has a length column for length sampling."
        },
    )


class CustomTrainer(Trainer):
    """A custom version of the trainer to make sure length sampling is mixed."""

    def __init__(
        self,
        model: PreTrainedModel | nn.Module | None = None,
        args: CustomTrainingArguments | None = None,
        *arguments: Any,
        test_indices_order: list[int] | None = None,
        train_indices_order: list[int] | None = None,
        train_grouped_indices: list[list[int]] | None = None,
        test_grouped_indices: list[list[int]] | None = None,
        **kwargs: Any,
    ) -> None:
        super().__init__(model, args, *arguments, **kwargs)  # type: ignore
        assert args is not None
        self.args = args

        self.test_indices_order = test_indices_order
        self.train_indices_order = train_indices_order

        self.test_grouped_indices = test_grouped_indices
        self.train_grouped_indices = train_grouped_indices

    def _get_train_sampler(self) -> torch.utils.data.Sampler | None:
        if self.train_dataset is None or not has_length(self.train_dataset):
            return None

        # Build the sampler.
        if self.args.group_by_length:
            if self.args.has_length_column:
                if hasattr(self.train_dataset, "metadata"):
                    lengths = [
                        v[self.args.length_column_name]
                        for k, v in self.train_dataset.metadata.items()  # type: ignore
                    ]
                else:
                    lengths = (
                        self.train_dataset[self.args.length_column_name]  # type: ignore
                        if self.args.length_column_name
                        in self.train_dataset.column_names  # type: ignore
                        else None
                    )
            else:
                lengths = None
            model_input_name = (
                self.processing_class.model_input_names[0]  # type: ignore
                if self.processing_class is not None
                else None
            )
            return CustomLengthGroupedSampler(
                self.args.train_batch_size * self.args.gradient_accumulation_steps,
                dataset=self.train_dataset,  # type: ignore
                lengths=lengths,
                model_input_name=model_input_name,
                mega_batch_mult=self.args.mega_batch_mult,
                indices_order=self.train_indices_order,
                grouped_indices=self.train_grouped_indices,
            )

        else:
            return RandomSampler(self.train_dataset)  # type: ignore

    def _get_eval_sampler(  # type: ignore
        self, eval_dataset: HFDataset
    ) -> torch.utils.data.Sampler | None:
        if eval_dataset is None or not has_length(eval_dataset):
            return None

        # Build the sampler.
        if self.args.group_by_length:
            if self.args.has_length_column:
                lengths = (
                    eval_dataset[self.args.length_column_name]
                    if self.args.length_column_name in eval_dataset.column_names
                    else None
                )
            else:
                lengths = None
            model_input_name = (
                self.tokenizer.model_input_names[0]
                if self.tokenizer is not None
                else None
            )
            return CustomLengthGroupedSampler(
                self.args.eval_batch_size,
                dataset=eval_dataset,
                lengths=lengths,
                model_input_name=model_input_name,
                indices_order=self.test_indices_order,
                grouped_indices=self.test_grouped_indices,
            )

        if self.args.world_size <= 1:
            return SequentialSampler(eval_dataset)
        else:
            return None


class CustomWav2Vec2Model(Wav2Vec2Model):
    """Custom model."""

    def forward(
        self,
        input_values: torch.Tensor | None,
        attention_mask: torch.Tensor | None = None,
        mask_time_indices: torch.FloatTensor | None = None,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
    ) -> tuple | Wav2Vec2BaseModelOutput:
        """Forward."""
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        extract_features = self.feature_extractor(input_values)
        extract_features = extract_features.transpose(1, 2)

        output_lengths = None
        if attention_mask is not None:
            # compute reduced attention_mask corresponding to feature vectors
            attention_mask, output_lengths = (
                self._get_feature_vector_attention_mask_and_lengths(
                    extract_features.shape[1],
                    attention_mask,  # type: ignore
                    add_adapter=False,  # type: ignore
                )
            )

        hidden_states, extract_features = self.feature_projection(extract_features)
        hidden_states = self._mask_hidden_states_with_lengths(
            hidden_states,
            mask_time_indices=mask_time_indices,
            output_lengths=output_lengths,
        )

        encoder_outputs = self.encoder(
            hidden_states,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = encoder_outputs[0]

        if self.adapter is not None:
            hidden_states = self.adapter(hidden_states)

        if not return_dict:
            return (hidden_states, extract_features) + encoder_outputs[1:]

        return Wav2Vec2BaseModelOutput(
            last_hidden_state=hidden_states,
            extract_features=extract_features,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )

    def _mask_hidden_states_with_lengths(
        self,
        hidden_states: torch.FloatTensor,
        mask_time_indices: torch.FloatTensor | None = None,
        output_lengths: torch.LongTensor | None = None,
    ) -> torch.Tensor:
        """Spec augment.

        Masks extracted features along time axis and/or along feature axis according to [SpecAugment](https://arxiv.org/abs/1904.08779).
        """
        # `config.apply_spec_augment` can set masking to False
        if not getattr(self.config, "apply_spec_augment", True):
            return hidden_states

        # generate indices & apply SpecAugment along time axis
        batch_size, sequence_length, hidden_size = hidden_states.size()

        if mask_time_indices is not None:
            # apply SpecAugment along time axis with given mask_time_indices
            hidden_states[mask_time_indices] = self.masked_spec_embed.to(
                hidden_states.dtype
            )
        elif self.config.mask_time_prob > 0 and self.training:
            mask_time_indices = _compute_mask_indices_with_lengths(  # type: ignore
                (batch_size, sequence_length),
                mask_prob=self.config.mask_time_prob,
                mask_length=self.config.mask_time_length,
                min_masks=self.config.mask_time_min_masks,
                output_lengths=output_lengths,
            )
            mask_time_indices = torch.tensor(
                mask_time_indices, device=hidden_states.device, dtype=torch.bool
            )  # type: ignore
            hidden_states[mask_time_indices] = self.masked_spec_embed.to(
                hidden_states.dtype
            )

        if self.config.mask_feature_prob > 0 and self.training:
            # generate indices & apply SpecAugment along feature axis
            mask_feature_indices = _compute_mask_indices_with_lengths(
                (batch_size, hidden_size),
                mask_prob=self.config.mask_feature_prob,
                mask_length=self.config.mask_feature_length,
                min_masks=self.config.mask_feature_min_masks,
                output_lengths=output_lengths,
            )
            mask_feature_indices = torch.tensor(
                mask_feature_indices, device=hidden_states.device, dtype=torch.bool
            )
            mask_feature_indices = mask_feature_indices[:, None].expand(
                -1, sequence_length, -1
            )
            hidden_states[mask_feature_indices] = 0

        return hidden_states

    def _get_feature_vector_attention_mask_and_lengths(
        self,
        feature_vector_length: int,
        attention_mask: torch.LongTensor,
        add_adapter: bool | None = None,
    ) -> tuple[torch.LongTensor, torch.LongTensor]:
        # Effectively attention_mask.sum(-1), but not inplace to be able to run
        # on inference mode.
        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]
        output_lengths: torch.LongTensor
        output_lengths = self._get_feat_extract_output_lengths(
            non_padded_lengths,  # type: ignore
            add_adapter=add_adapter,  # type: ignore
        )
        output_lengths = output_lengths.to(torch.long)  # type: ignore
        attention_mask = torch.arange(
            feature_vector_length,
            device=attention_mask.device,  # type: ignore
        ).unsqueeze(0) < output_lengths.unsqueeze(1)
        return attention_mask, output_lengths


class FlacDataset(TorchDataset):
    """A wrapping dataset for caching audio as local flac files."""

    def __init__(
        self,
        inner_dataset: HFDataset | TorchDataset,
        cache_path: str | Path,
        sample_rate: int,
        metadata: dict[int, dict[str, Any]] | None = None,
    ) -> None:
        self._inner_dataset = inner_dataset
        self.cache_path = Path(cache_path)
        self.metadata_path = self.cache_path / "metadata.parquet"
        if metadata is None:
            metadata = self.load_metadata()
        self.metadata = metadata
        self.sample_rate = sample_rate

    def complete_metadata(self) -> None:
        """Make sure that the metadata is complete."""
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=2 * min(32, os.cpu_count() + 4)  # type: ignore
        ) as executor:
            self.set_metadata(self.load_metadata())
            if len(self.metadata) < len(self):
                for _ in tqdm(
                    executor.map(lambda i: self.validate_item(i), range(len(self))),
                    total=len(self),
                ):
                    pass
                self.save_metadata()

    def validate_item(self, index: int) -> None:
        """Validate item."""
        item = self[index]
        for _ in range(3):
            if len(item["input_values"]) == item["length"]:
                return

            Path(item["file_path"]).unlink(missing_ok=True)
            item = self[index]
        assert len(item["input_values"]) == item["length"], (
            f"Length mismatch with index [{index}]"
        )

    def save_metadata(self) -> None:
        """Save the metadata as a parquet."""
        pl.from_dicts(
            [
                dict(i=i, **self.metadata[i])
                for i in range(len(self))
                if i in self.metadata
            ]
        ).sort(by="i").write_parquet(self.metadata_path)

    def __getattr__(self, name: str) -> Any:
        """Delegate to the inner if it has the attribute."""
        result = getattr(self._inner_dataset, name)
        if issubclass(type(result), HFDataset) or issubclass(
            type(result), TorchDataset
        ):
            result = FlacDataset(
                result, self.cache_path, self.sample_rate, self.metadata
            )
        return result

    def load_metadata(self) -> dict[int, dict[str, Any]]:
        """Load metadata from a file."""
        if self.metadata_path.exists():
            df = pl.read_parquet(self.metadata_path).sort(by="i")
            return {
                row["i"]: {k: v for k, v in row.items() if k != "i"}
                for row in df.to_dicts()
            }
        else:
            return dict()

    def set_metadata(self, metadata: dict[int, dict[str, Any]]) -> None:
        """Set the current metadata value."""
        self.metadata = metadata

    def __len__(self) -> int:
        """Propegates the length of the inner dataset."""
        return len(self._inner_dataset)  # type: ignore

    def __getitems__(self, keys: list) -> list:
        """Can be used to get a batch using a list of integers indices."""
        return [self[k] for k in keys]

    def __getitem__(self, index: int | str) -> dict[str, Any] | Any:
        """Return the item corresponding to the index while caching both metadata and audio to files."""
        if isinstance(index, str):
            return self._inner_dataset[index]

        item = None

        index = index % len(self)

        padded_index = str(index).zfill(len(str(len(self._inner_dataset))))  # type: ignore
        flac_path = self.cache_path / f"{padded_index}.flac"
        if not flac_path.exists():
            if item is None:
                item = self._inner_dataset[index]
            samples = item["input_values"]
            self._save_flac(flac_path, samples)

        samples = self._load_flac(flac_path)

        if index in self.metadata:
            item_metadata = self.metadata[index]
        else:
            if item is None:
                item = self._inner_dataset[index]
            item_metadata = {k: v for k, v in item.items() if k != "input_values"}
            item_metadata["indices"] = index
            item_metadata["file_paths"] = str(flac_path)
            item_metadata["file_sizes"] = flac_path.stat().st_size
            self.metadata[index] = item_metadata

        item = dict(
            input_values=samples,
            **item_metadata,
        )
        return item

    def _save_flac(self, flac_path: Path, samples: list[int]) -> None:
        _samples = torch.tensor(samples)
        _samples = _samples - _samples.min()
        _samples = _samples / _samples.max()
        _samples = (_samples * (2**31 - 2**24)).to(torch.int32)
        ta.save(
            str(flac_path.absolute()),
            _samples[None, :],
            self.sample_rate,
            format="flac",
            bits_per_sample=24,
            encoding="PCM_S",
        )

    def _load_flac(self, flac_path: Path) -> list[int]:
        samples, sr = ta.load(flac_path.absolute(), normalize=False, format="flac")
        assert sr == self.sample_rate
        assert samples.shape[0] == 1
        samples = samples[0, :]
        samples = (samples / (2**30 - 2**24)).to(torch.float32)
        samples = samples - samples.mean()
        samples = samples / samples.std()
        return samples.tolist()


class TarS3Dataset(TorchDataset):
    """A wrapping dataset for caching files to s3."""

    def __init__(
        self,
        inner_dataset: FlacDataset,
        cache_path: str | Path,
        s3_client: S3Client,
        s3_client_v2: S3Client,
        cache_bucket: str,
        indices_order: list[int],
        max_tar_bytes: int,
        syncs_per_group: int,
    ) -> None:
        self._inner_dataset = inner_dataset
        self.cache_path = Path(cache_path)
        self.s3_client = s3_client
        self.s3_client_v2 = s3_client_v2
        self.cache_bucket = cache_bucket
        self._indices_order = indices_order
        self.max_tar_bytes = max_tar_bytes
        self.syncs_per_group = syncs_per_group

        for _ in tqdm(list(range(1))):
            self.sync_metadata()
        inner_dataset.complete_metadata()
        for _ in tqdm(list(range(1))):
            self.sync_metadata()

        file_sizes = {i: v["file_sizes"] for i, v in inner_dataset.metadata.items()}
        self._file_sizes = file_sizes

        cum_sizes = np.array(
            [x[1] for x in sorted(file_sizes.items(), key=lambda x: x[0])]
        ).cumsum()
        i = 0
        prev_i = 0
        self.grouped_indices = []
        while (
            (pos_sizes := ((cum_sizes := cum_sizes - self.max_tar_bytes) > 0))
            .any()
            .item()
        ):
            prev_i = i
            i = np.diff(pos_sizes).argmax().item()

            self.grouped_indices.append(indices_order[prev_i:i])
        self.grouped_indices.append(indices_order[i:])

        self._indices_groups = {
            v: i for i, vals in enumerate(self.grouped_indices) for v in vals
        }
        self._indices_flac_paths = {i: self._get_flac_path(i) for i in indices_order}

        self.sync_groups = [
            [
                group_indices[
                    i * len(group_indices) // syncs_per_group : (i + 1)
                    * len(group_indices)
                    // syncs_per_group
                ]
                for i in range(syncs_per_group)
            ]
            for group_indices in self.grouped_indices
        ]
        self.sync_indices = [
            sync_group[0] for group in self.sync_groups for sync_group in group
        ]

        self.sync_safe_groups = [
            sync_safe_group
            for group in self.sync_groups
            for sync_group in group
            for sync_safe_group in [
                sync_group[: len(sync_group) // 2],
                sync_group[len(sync_group) // 2 :],
            ]
        ]

        if not self._bucket_exists(self.cache_bucket):
            self._create_bucket(self.cache_bucket)

        for _ in tqdm(list(range(1))):
            self.sync_group(0)

    def __getattr__(self, name: str) -> Any:
        """Delegate to the inner if it has the attribute."""
        result = getattr(self._inner_dataset, name)
        if issubclass(type(result), HFDataset) or issubclass(
            type(result), TorchDataset
        ):
            result = TarS3Dataset(
                result,
                self.cache_path,
                self.s3_client,
                self.s3_client_v2,
                self.cache_bucket,
                self._indices_order,
                self.max_tar_bytes,
                self.syncs_per_group,
            )
        return result

    def _bucket_exists(self, bucket: str) -> bool:
        try:
            head_metadata = self.s3_client.head_bucket(Bucket=bucket)[
                "ResponseMetadata"
            ]
            return (
                ("HTTPStatusCode" in head_metadata)
                and (head_metadata["HTTPStatusCode"] == 200)
                and ("HTTPHeaders" in head_metadata)
                and ("content-length" in head_metadata["HTTPHeaders"])
                and (int(head_metadata["HTTPHeaders"]["content-length"]) == 0)
            )
        except ClientError:
            return False

    def _create_bucket(self, bucket: str) -> None:
        self.s3_client.create_bucket(Bucket=bucket)

    def _exists(self, name: str) -> bool:
        try:
            head_metadata = self.s3_client.head_object(
                Bucket=self.cache_bucket, Key=f"{name}.tar.gz"
            )["ResponseMetadata"]
            return (
                ("HTTPStatusCode" in head_metadata)
                and (head_metadata["HTTPStatusCode"] == 200)
                and ("HTTPHeaders" in head_metadata)
                and ("content-length" in head_metadata["HTTPHeaders"])
                and (int(head_metadata["HTTPHeaders"]["content-length"]) > 0)
            )
        except ClientError:
            return False

    def group_exists(self, group: int) -> bool:
        """Check whether a group exists in s3 cache or not."""
        padded_group = str(group % len(self.grouped_indices)).zfill(
            len(str(len(self.grouped_indices)))
        )  # type: ignore
        return self._exists(padded_group)

    def _upload(self, files: list[Path], name: str) -> None:
        if self._exists(name):
            return
        with tempfile.TemporaryFile() as f:
            with tarfile.open(fileobj=f, mode="w:gz") as tar:
                for file in files:
                    if file.exists():
                        tar.add(str(file.absolute()), arcname=file.name)
                    else:
                        print(f"Warning: {file} does not exist and will be skipped.")
            f.seek(0)
            self.s3_client_v2.upload_fileobj(
                Fileobj=f, Bucket=self.cache_bucket, Key=f"{name}.tar.gz"
            )

    def _download(self, name: str) -> None:
        with tempfile.TemporaryFile() as f:
            self.s3_client.download_fileobj(
                Bucket=self.cache_bucket, Key=f"{name}.tar.gz", Fileobj=f
            )
            f.seek(0)
            with tarfile.open(fileobj=f, mode="r:gz") as tar:
                tar.extractall(path=str(self.cache_path.absolute()), filter="data")

    def __len__(self) -> int:
        """Propegates the length of the inner dataset."""
        return len(self._inner_dataset)  # type: ignore

    def __getitems__(self, keys: list) -> list:
        """Can be used to get a batch using a list of integers indices."""
        return [self[k] for k in keys]

    def _get_flac_path(self, index: int) -> Path:
        """Get the flac path of an index."""
        padded_index = str(index).zfill(len(str(len(self._inner_dataset))))  # type: ignore
        flac_path = self.cache_path / f"{padded_index}.flac"
        return flac_path

    def __getitem__(self, index: int | str) -> dict[str, Any] | Any:
        """Return the item corresponding to the index while caching both metadata and audio to files."""
        if isinstance(index, str):
            return self._inner_dataset[index]

        index = index % len(self)

        current_group = self._indices_groups[index]

        if index in self.sync_indices:
            self.start_sync(current_group)

        return self._inner_dataset[index]

    def start_sync(self, current_group: int) -> None:
        """Start a parallel sync using threading."""
        thread = threading.Thread(
            target=self.sync_adjacent_groups, args=(current_group,)
        )
        thread.start()

    def sync_adjacent_groups(self, current_group: int) -> None:
        """Sync adjacent groups."""
        prev_group = (current_group - 1) % len(self.grouped_indices)
        next_group = (current_group + 1) % len(self.grouped_indices)

        self.sync_group(prev_group)
        self.sync_group(current_group)
        self.sync_group(next_group)

    def sync_group(self, group: int) -> None:
        """Sync the group of local files with s3 tar."""
        padded_group = str(group).zfill(len(str(len(self.grouped_indices))))  # type: ignore
        group_flac_paths = [
            self._indices_flac_paths[i] for i in self.grouped_indices[group]
        ]
        if all(p.exists() for p in group_flac_paths):
            self._upload(group_flac_paths, padded_group)
        elif self._exists(padded_group):
            self._download(padded_group)

    def sync_all_groups(self) -> None:
        """Sync all groups."""
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=2 * min(32, os.cpu_count() + 4)  # type: ignore
        ) as executor:
            if any(
                (not self.group_exists(i)) for i in range(len(self.grouped_indices))
            ):
                for _ in tqdm(
                    executor.map(
                        self.sync_group,
                        range(len(self.grouped_indices)),
                    ),
                    total=len(self.grouped_indices),
                ):
                    pass

    def sync_metadata(self) -> None:
        """Sync the metadata file with s3."""
        self.sync_file(self.cache_path / "metadata.parquet")

    def sync_file(self, file: Path) -> None:
        """Sync a single file."""
        if file.exists():
            self._upload([file], file.stem)
        elif self._exists(file.stem):
            self._download(file.stem)


class ResizedDataset(TorchDataset):
    """A wrapping dataset for caching files to s3."""

    def __init__(self, inner_dataset: HFDataset | TorchDataset, size: int) -> None:
        self._inner_dataset = inner_dataset
        self._size = size

    def __getattr__(self, name: str) -> Any:
        """Delegate to the inner if it has the attribute."""
        result = getattr(self._inner_dataset, name)
        if issubclass(type(result), HFDataset) or issubclass(
            type(result), TorchDataset
        ):
            result = ResizedDataset(result, self._size)
        elif self._is_resizable(result):
            result = self._resize(result)
        return result

    def __len__(self) -> int:
        """Return the size as specified."""
        return self._size

    @property
    def total_length(self) -> int:
        """Return the size as specified."""
        return self._size

    def __getitems__(self, keys: list) -> list:
        """Can be used to get a batch using a list of integers indices."""
        return [self[k] for k in keys]

    def __getitem__(self, index: int | str) -> dict[str, Any] | Any:
        """Return the item corresponding to the index while caching both metadata and audio to files."""
        if isinstance(index, str):
            result = self._inner_dataset[index]
            if self._is_resizable(result):  # type: ignore
                result = self._resize(result)
            return result

        if index > self._size:
            raise IndexError()
        inner_index = index % len(self._inner_dataset)  # type: ignore
        return self._inner_dataset[inner_index]

    def _is_resizable(self, x: Any) -> bool:
        return isinstance(x, list) and (len(x) == len(self._inner_dataset))  # type: ignore

    def _resize(self, values: list) -> list:
        """Resize a list to the custom size."""
        return [values[i % len(values)] for i in range(self._size)]


GB = 1_000_000_000


def prepare_cached_dataset(
    dataset: TorchDataset | HFDataset,
    sample_rate: int,
    cache_path: str,
    cache_bucket: str,
    s3_client: S3Client,
    s3_client_v2: S3Client,
    tar_size_gb: float | int = 1,
    syncs_per_group: int = 3,
) -> TarS3Dataset:
    """Wrap a dataset with S3 caching and prepare for the first training."""
    dataset = FlacDataset(dataset, cache_path, sample_rate)
    indices_order = list(range(len(dataset)))
    dataset = TarS3Dataset(
        dataset,
        cache_path,
        s3_client,
        s3_client_v2,
        cache_bucket,
        indices_order,
        max_tar_bytes=int(tar_size_gb * GB),
        syncs_per_group=syncs_per_group,
    )
    # dataset.sync_all_groups()
    return dataset


if is_flash_attn_2_available():
    print("Flash-attn 2 Available.")
else:
    print("WARNING: Can't access flash-attn 2!")

print(f"torchaudio backends: {ta.list_audio_backends()}")

seed = 42
data_seed = 42
set_seed(seed)

common_voice_train = load_dataset(
    "mozilla-foundation/common_voice_17_0",
    "tr",
    # split="train+validation",
    split="train",
    token=True,
    trust_remote_code=True,
    # streaming=True,
)
assert isinstance(common_voice_train, HFDataset)
common_voice_test = load_dataset(
    "mozilla-foundation/common_voice_17_0",
    "tr",
    split="test",
    token=True,
    trust_remote_code=True,
    # streaming=True,
)
assert isinstance(common_voice_test, HFDataset)


common_voice_train = common_voice_train.remove_columns(
    [
        "accent",
        "age",
        "client_id",
        "down_votes",
        "gender",
        "locale",
        "segment",
        "up_votes",
    ]
)
common_voice_test = common_voice_test.remove_columns(
    [
        "accent",
        "age",
        "client_id",
        "down_votes",
        "gender",
        "locale",
        "segment",
        "up_votes",
    ]
)

chars_to_remove_regex = r"[`,?\.!\-;:\"“%‘”�()…’]"  # noqa: RUF001
Batch = dict[str, Any]
ur = uroman.Uroman()
target_lang = "tur"


def uromanize(batch: Batch) -> Batch:
    """Uromanize text."""
    clean_string = re.sub(chars_to_remove_regex, "", batch["sentence"]).lower()
    batch["sentence"] = ur.romanize_string(clean_string, lcode=target_lang)
    return batch


common_voice_train = common_voice_train.map(uromanize)
common_voice_test = common_voice_test.map(uromanize)

Vocab = dict[str, Any]


def extract_all_chars(batch: Batch) -> Vocab:
    """Extract all chars from batch."""
    all_text = " ".join(batch["sentence"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}


sample_rate = 16_000

common_voice_train = common_voice_train.cast_column(
    "audio", Audio(sampling_rate=sample_rate)
)
common_voice_test = common_voice_test.cast_column(
    "audio", Audio(sampling_rate=sample_rate)
)

rand_int = random.randint(0, len(common_voice_train) - 1)

print("Target text:", common_voice_train[rand_int]["sentence"])
print("Input array shape:", common_voice_train[rand_int]["audio"]["array"].shape)
print("Sampling rate:", common_voice_train[rand_int]["audio"]["sampling_rate"])


def prepare_dataset(batch: Batch) -> Batch:
    """Prepare dataset."""
    audio = batch["audio"]
    batch["input_values"] = processor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_values[0]
    batch["length"] = len(batch["input_values"])

    batch["labels"] = processor(text=batch["sentence"]).input_ids  # type: ignore
    return batch


common_voice_train = common_voice_train.map(
    prepare_dataset, remove_columns=common_voice_train.column_names
)
common_voice_test = common_voice_test.map(
    prepare_dataset, remove_columns=common_voice_test.column_names
)

access_key = os.getenv("HETZNER_ACCESS_KEY")
secret_key = os.getenv("HETZNER_SECRET_KEY")
endpoint = os.getenv("HETZNER_ENDPOINT")
region = os.getenv("HETZNER_REGION")


s3_client, s3_client_v2 = [
    boto3.client(
        "s3",
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        endpoint_url=endpoint,
        aws_session_token=None,
        config=boto3.session.Config(  # type: ignore
            retries={"max_attempts": 1, "mode": "standard"},
            signature_version=signature_version,
            region_name=region,
            s3=dict(addressing_style="virtual"),
        ),
    )
    for signature_version in ["s3v4", "s3"]
]

test_cache_path = f"./.app_cache/{data_seed}/test_set/"
train_cache_path = f"./.app_cache/{data_seed}/train_set/"
Path(test_cache_path).mkdir(parents=True, exist_ok=True)
Path(train_cache_path).mkdir(parents=True, exist_ok=True)

repo_name = "mms-300m-turkish"

test_cache_bucket = f"{repo_name}-cache-{data_seed}-test-set"
train_cache_bucket = f"{repo_name}-cache-{data_seed}-train-set"

eval_size = 20_000
train_size = 100_000

eval_limit = 4_000

common_voice_test = common_voice_test.shuffle(seed=data_seed)
common_voice_train = common_voice_train.shuffle(seed=data_seed)

common_voice_test = ResizedDataset(common_voice_test, eval_size)
common_voice_train = ResizedDataset(common_voice_train, train_size)

orig_common_voice_test = common_voice_test
orig_common_voice_train = common_voice_train

common_voice_test = prepare_cached_dataset(
    common_voice_test,
    sample_rate,
    test_cache_path,
    test_cache_bucket,
    s3_client,
    s3_client_v2,
)
common_voice_train = prepare_cached_dataset(
    common_voice_train,
    sample_rate,
    train_cache_path,
    train_cache_bucket,
    s3_client,
    s3_client_v2,
)

common_voice_test = ResizedDataset(common_voice_test, eval_limit)

test_indices_order = list(range(len(common_voice_test)))
train_indices_order = list(range(len(common_voice_train)))

hf_repo = "mms-meta/mms-zeroshot-300m"
print(f"hf_repo: {hf_repo}")
tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(hf_repo)
vocab_dict = tokenizer.vocab

new_vocab_dict = {target_lang: vocab_dict}

with open(".output/vocab.json", "w") as vocab_file:
    json.dump(new_vocab_dict, vocab_file)


tokenizer: Wav2Vec2CTCTokenizer = Wav2Vec2CTCTokenizer.from_pretrained(
    ".output/",
    unk_token=tokenizer.unk_token,
    pad_token=tokenizer.pad_token,
    word_delimiter_token=tokenizer.word_delimiter_token,
    bos_token=tokenizer.bos_token,
    eos_token=tokenizer.eos_token,
    target_lang=target_lang,
)

tokenizer.push_to_hub(repo_name)  # type: ignore

feature_extractor = Wav2Vec2FeatureExtractor(
    feature_size=1,
    sampling_rate=sample_rate,
    padding_value=0.0,
    do_normalize=True,
    return_attention_mask=True,
)


@runtime_checkable
class HasCustomFields(Protocol):
    """Just for pyright type checking."""

    tokenizer: Wav2Vec2CTCTokenizer
    feature_extractor: Wav2Vec2FeatureExtractor


processor: Wav2Vec2Processor = Wav2Vec2Processor(
    feature_extractor=feature_extractor, tokenizer=tokenizer
)
assert isinstance(processor, HasCustomFields) and isinstance(
    processor, Wav2Vec2Processor
)


data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)

wer_metric = load("wer")
cer_metric = load("cer")


Metrics = dict[str, Any]


def compute_metrics(pred: EvalPrediction) -> Metrics:
    """Compute metrics."""
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    assert isinstance(processor, HasCustomFields)
    if isinstance(pred.label_ids, tuple):
        pred.label_ids = pred.label_ids[0]
    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id  # type: ignore

    pred_str = processor.batch_decode(pred_ids)
    # we do not want to group tokens when computing the metrics
    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    for i in range(100):
        print(f'({i}) pred: "{pred_str[i]}"')
        print(f'({i}) targ: "{label_str[i]}"')
    return {"wer": wer, "cer": cer}


model = CustomWav2Vec2ForCTC.from_pretrained(
    hf_repo,
    hidden_dropout=0.0,
    activation_dropout=0.0,
    attention_dropout=0.0,
    feat_proj_dropout=0.0,
    feat_quantizer_dropout=0.0,
    final_dropout=0.0,
    layerdrop=0.0,
    ctc_loss_reduction="mean",
    vocab_size=len(processor.tokenizer),
    adapter_attn_dim=len(processor.tokenizer),
    ignore_mismatched_sizes=True,
    # attn_implementation="flash_attention_2",
    # attn_implementation="sdpa",
)

model.init_adapter_layers()

model.freeze_base_model()
model.freeze_feature_encoder()

adapter_weights = model._get_adapters()
for param in adapter_weights.values():
    param.requires_grad = True

comet_ml.login(project_name=os.getenv("WANDB_PROJECT"))

num_train_epochs = 1
effective_batch_size = 16
per_device_train_batch_size = 4
per_device_eval_batch_size = 8
num_devices = 1
warmup_ratio = 0.0
decay_ratio = 1.0
learning_rate = 1e-4

global_batch_size = per_device_train_batch_size * num_devices
accumulation_steps = effective_batch_size // global_batch_size

num_training_steps = (
    len(common_voice_train) // effective_batch_size  # type: ignore
) * num_train_epochs

lr_scheduler_kwargs = dict(num_decay_steps=int(decay_ratio * num_training_steps))

training_args = CustomTrainingArguments(
    seed=seed,
    data_seed=data_seed,
    report_to=["comet_ml", "wandb"],
    output_dir=f".output/{repo_name}",
    run_name=repo_name,
    group_by_length=True,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_eval_batch_size,
    gradient_accumulation_steps=accumulation_steps,
    eval_strategy="steps",
    num_train_epochs=num_train_epochs,
    mega_batch_mult=100,
    dataloader_num_workers=4,
    # dataloader_num_workers=0,
    dataloader_drop_last=True,
    gradient_checkpointing=True,
    fp16=False,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=100,
    eval_on_start=True,
    logging_first_step=True,
    learning_rate=learning_rate,
    lr_scheduler_type="warmup_stable_decay",
    warmup_steps=int(warmup_ratio * num_training_steps),
    lr_scheduler_kwargs=lr_scheduler_kwargs,
    weight_decay=0.01,
    save_total_limit=2,
    # push_to_hub=True,
    push_to_hub=False,
    logging_nan_inf_filter=True,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    log_level="info",
    # torch_compile=True,
    torch_compile=False,
)

trainer = CustomTrainer(
    model=model,  # type: ignore
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    eval_dataset=common_voice_test,
    train_dataset=common_voice_train,
    processing_class=processor.feature_extractor,
    test_indices_order=test_indices_order,
    train_indices_order=train_indices_order,
    test_grouped_indices=common_voice_test.sync_safe_groups,
    train_grouped_indices=common_voice_train.sync_safe_groups,
)

trainer.train()
trainer.evaluate()

wandb.finish()
comet_ml.end()

adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(target_lang)
assert training_args.output_dir is not None
adapter_file = os.path.join(training_args.output_dir, adapter_file)

safe_save_file(model._get_adapters(), adapter_file, metadata={"format": "pt"})

trainer.push_to_hub()

model_id = f"Kellner/{repo_name}"

model = CustomWav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang).to(
    "cuda"  # type: ignore
)
_processor = Wav2Vec2Processor.from_pretrained(model_id)
assert isinstance(_processor, HasCustomFields) and isinstance(
    _processor, Wav2Vec2Processor
)
processor = _processor

processor.tokenizer.set_target_lang(target_lang)  # type: ignore

common_voice_test_tr: HFDataset = load_dataset(
    "mozilla-foundation/common_voice_17_0",
    "tr",
    data_dir="./cv-corpus-17.0-2024-03-20",
    split="test",
    token=True,
    trust_remote_code=True,
    # streaming=True,
)  # type: ignore
common_voice_test_tr = common_voice_test_tr.cast_column(
    "audio", Audio(sampling_rate=sample_rate)
)

input_dict = processor(
    common_voice_test_tr[0]["audio"]["array"],
    sampling_rate=sample_rate,
    return_tensors="pt",
    padding=True,
)

logits = model(input_dict.input_values.to("cuda")).logits

pred_ids = torch.argmax(logits, dim=-1)[0]

print("Prediction:")
print(processor.decode(pred_ids))

print("\nReference:")
print(common_voice_test_tr[0]["sentence"].lower())
