"""Fine_Tune_MMS_on_Common_Voice.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_MMS_on_Common_Voice.ipynb
"""

import os
from pathlib import Path

import boto3
import comet_ml
import torch
import torchaudio as ta
import wandb
from datasets import Audio, load_dataset
from datasets import Dataset as HFDataset
from safetensors.torch import save_file as safe_save_file
from transformers import (
    Trainer,
    Wav2Vec2Processor,
    set_seed,
)
from transformers.models.wav2vec2.modeling_wav2vec2 import (
    WAV2VEC2_ADAPTER_SAFE_FILE,
)
from transformers.utils import is_flash_attn_2_available

from finetune_setup_example.custom_datasets import prepare_cached_dataset
from finetune_setup_example.custom_datasets.lazy import LazyDataset
from finetune_setup_example.custom_datasets.resized import ResizedDataset
from finetune_setup_example.custom_hf.training_args import CustomTrainingArguments
from finetune_setup_example.custom_wav2vec2.wav2vec2_for_ctc import (
    CustomWav2Vec2ForCTC,
)
from finetune_setup_example.specific_datasets.common_voice import (
    load_common_voice_for_wav2vec2,
)
from finetune_setup_example.specific_wav2vec2.model import load_wav2vec2_for_adaptuning
from finetune_setup_example.specific_wav2vec2.processor import (
    HasCustomFields,
    create_wav2vec2_processor,
)
from finetune_setup_example.specific_wav2vec2.trainer import create_trainer

if is_flash_attn_2_available():
    print("Flash-attn 2 Available.")
else:
    print("WARNING: Can't access flash-attn 2!")

print(f"torchaudio backends: {ta.list_audio_backends()}")

seed = 42
data_seed = 42
set_seed(seed)

target_lang = "tur"
sample_rate = 16_000
base_hf_repo = "mms-meta/mms-zeroshot-300m"
target_hf_repo = "mms-300m-turkish"
print(f"base_hf_repo: {base_hf_repo}")
print(f"target_hf_repo: {target_hf_repo}")


processor = create_wav2vec2_processor(
    target_lang=target_lang,
    sample_rate=sample_rate,
    base_hf_repo=base_hf_repo,
    target_hf_repo=target_hf_repo,
)
assert isinstance(processor, HasCustomFields)


common_voice_train = LazyDataset(
    lambda: load_common_voice_for_wav2vec2(
        processor=processor,
        target_lang=target_lang,
        sample_rate=sample_rate,
        split="train",
        data_seed=data_seed,
    ),
    35147,
)
common_voice_test = LazyDataset(
    lambda: load_common_voice_for_wav2vec2(
        processor=processor,
        target_lang=target_lang,
        sample_rate=sample_rate,
        split="test",
        data_seed=data_seed,
    ),
    11290,
)

eval_size = 20_000
train_size = 100_000

eval_limit = 4_000

common_voice_test = ResizedDataset(common_voice_test, eval_size)
common_voice_train = ResizedDataset(common_voice_train, train_size)

access_key = os.getenv("HETZNER_ACCESS_KEY")
secret_key = os.getenv("HETZNER_SECRET_KEY")
endpoint = os.getenv("HETZNER_ENDPOINT")
region = os.getenv("HETZNER_REGION")

s3_client, s3_client_v2 = [
    boto3.client(
        "s3",
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        endpoint_url=endpoint,
        aws_session_token=None,
        config=boto3.session.Config(  # type: ignore
            retries={"max_attempts": 1, "mode": "standard"},
            signature_version=signature_version,
            region_name=region,
            s3=dict(addressing_style="virtual"),
        ),
    )
    for signature_version in ["s3v4", "s3"]
]

test_cache_path = f"./.app_cache/{data_seed}/test_set/"
train_cache_path = f"./.app_cache/{data_seed}/train_set/"
Path(test_cache_path).mkdir(parents=True, exist_ok=True)
Path(train_cache_path).mkdir(parents=True, exist_ok=True)

test_cache_bucket = f"{target_hf_repo}-cache-{data_seed}-test-set"
train_cache_bucket = f"{target_hf_repo}-cache-{data_seed}-train-set"

orig_common_voice_test = common_voice_test
orig_common_voice_train = common_voice_train

common_voice_test = prepare_cached_dataset(
    common_voice_test,
    sample_rate,
    test_cache_path,
    test_cache_bucket,
    s3_client,
    s3_client_v2,
)
common_voice_train = prepare_cached_dataset(
    common_voice_train,
    sample_rate,
    train_cache_path,
    train_cache_bucket,
    s3_client,
    s3_client_v2,
)

common_voice_test = ResizedDataset(common_voice_test, eval_limit)


model = load_wav2vec2_for_adaptuning(base_hf_repo=base_hf_repo, processor=processor)

num_train_epochs = 1
effective_batch_size = 16
per_device_train_batch_size = 4
per_device_eval_batch_size = 8
num_devices = 1
warmup_ratio = 0.0
decay_ratio = 1.0
learning_rate = 1e-2

global_batch_size = per_device_train_batch_size * num_devices
accumulation_steps = effective_batch_size // global_batch_size

num_training_steps = (
    len(common_voice_train) // effective_batch_size  # type: ignore
) * num_train_epochs

lr_scheduler_kwargs = dict(num_decay_steps=int(decay_ratio * num_training_steps))

training_args = CustomTrainingArguments(
    seed=seed,
    data_seed=data_seed,
    report_to=["comet_ml", "wandb"],
    output_dir=f".output/{target_hf_repo}",
    run_name=target_hf_repo,
    group_by_length=True,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_eval_batch_size,
    gradient_accumulation_steps=accumulation_steps,
    eval_strategy="steps",
    num_train_epochs=num_train_epochs,
    mega_batch_mult=100,
    dataloader_num_workers=4,
    # dataloader_num_workers=0,
    dataloader_drop_last=True,
    gradient_checkpointing=True,
    fp16=False,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=100,
    eval_on_start=True,
    logging_first_step=True,
    learning_rate=learning_rate,
    lr_scheduler_type="warmup_stable_decay",
    warmup_steps=int(warmup_ratio * num_training_steps),
    lr_scheduler_kwargs=lr_scheduler_kwargs,
    weight_decay=0.01,
    save_total_limit=2,
    # push_to_hub=True,
    push_to_hub=False,
    logging_nan_inf_filter=True,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    log_level="info",
    # torch_compile=True,
    torch_compile=False,
)


def train(trainer: Trainer) -> None:
    """Train with cometml and wandb."""
    comet_ml.login(project_name=os.getenv("WANDB_PROJECT"))
    wandb.init(dir="./.wandb")

    trainer.train()
    trainer.evaluate()

    comet_ml.end()
    wandb.finish()


trainer = create_trainer(
    model=model,
    training_args=training_args,
    common_voice_test=common_voice_test,
    common_voice_train=common_voice_train,
    processor=processor,
)
train(trainer)


adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(target_lang)
assert training_args.output_dir is not None
adapter_file = os.path.join(training_args.output_dir, adapter_file)

safe_save_file(model._get_adapters(), adapter_file, metadata={"format": "pt"})

trainer.push_to_hub()

model_id = f"Kellner/{target_hf_repo}"

model = CustomWav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang).to(
    "cuda"  # type: ignore
)
_processor = Wav2Vec2Processor.from_pretrained(model_id)
assert isinstance(_processor, HasCustomFields) and isinstance(
    _processor, Wav2Vec2Processor
)
processor = _processor

processor.tokenizer.set_target_lang(target_lang)  # type: ignore

common_voice_test_tr: HFDataset = load_dataset(
    "mozilla-foundation/common_voice_17_0",
    "tr",
    data_dir="./cv-corpus-17.0-2024-03-20",
    split="test",
    token=True,
    trust_remote_code=True,
    # streaming=True,
)  # type: ignore
common_voice_test_tr = common_voice_test_tr.cast_column(
    "audio", Audio(sampling_rate=sample_rate)
)

input_dict = processor(
    common_voice_test_tr[0]["audio"]["array"],
    sampling_rate=sample_rate,
    return_tensors="pt",
    padding=True,
)

logits = model(input_dict.input_values.to("cuda")).logits

pred_ids = torch.argmax(logits, dim=-1)[0]

print("Prediction:")
print(processor.decode(pred_ids))

print("\nReference:")
print(common_voice_test_tr[0]["sentence"].lower())
