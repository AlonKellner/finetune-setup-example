"""Fine_Tune_MMS_on_Common_Voice.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_MMS_on_Common_Voice.ipynb
"""

import json
import os
import random
import re
from pathlib import Path
from typing import Any, Protocol, runtime_checkable

import boto3
import comet_ml
import numpy as np
import torch
import torchaudio as ta
import uroman
import wandb
from datasets import Audio, load_dataset
from datasets import Dataset as HFDataset
from evaluate import load
from safetensors.torch import save_file as safe_save_file
from transformers import (
    EvalPrediction,
    Wav2Vec2CTCTokenizer,
    Wav2Vec2FeatureExtractor,
    Wav2Vec2Processor,
    set_seed,
)
from transformers.models.wav2vec2.modeling_wav2vec2 import (
    WAV2VEC2_ADAPTER_SAFE_FILE,
)
from transformers.utils import is_flash_attn_2_available

from src.finetune_setup_example.custom_datasets import prepare_cached_dataset
from src.finetune_setup_example.custom_datasets.resized import ResizedDataset
from src.finetune_setup_example.custom_hf.trainer import CustomTrainer
from src.finetune_setup_example.custom_hf.training_args import CustomTrainingArguments
from src.finetune_setup_example.custom_wav2vec2.ctc_collator import (
    DataCollatorCTCWithPadding,
)
from src.finetune_setup_example.custom_wav2vec2.wav2vec2_for_ctc import (
    CustomWav2Vec2ForCTC,
)

if is_flash_attn_2_available():
    print("Flash-attn 2 Available.")
else:
    print("WARNING: Can't access flash-attn 2!")

print(f"torchaudio backends: {ta.list_audio_backends()}")

seed = 42
data_seed = 42
set_seed(seed)

common_voice_train = load_dataset(
    "mozilla-foundation/common_voice_17_0",
    "tr",
    # split="train+validation",
    split="train",
    token=True,
    trust_remote_code=True,
    # streaming=True,
)
assert isinstance(common_voice_train, HFDataset)
common_voice_test = load_dataset(
    "mozilla-foundation/common_voice_17_0",
    "tr",
    split="test",
    token=True,
    trust_remote_code=True,
    # streaming=True,
)
assert isinstance(common_voice_test, HFDataset)


common_voice_train = common_voice_train.remove_columns(
    [
        "accent",
        "age",
        "client_id",
        "down_votes",
        "gender",
        "locale",
        "segment",
        "up_votes",
    ]
)
common_voice_test = common_voice_test.remove_columns(
    [
        "accent",
        "age",
        "client_id",
        "down_votes",
        "gender",
        "locale",
        "segment",
        "up_votes",
    ]
)

chars_to_remove_regex = r"[`,?\.!\-;:\"“%‘”�()…’]"  # noqa: RUF001
Batch = dict[str, Any]
ur = uroman.Uroman()
target_lang = "tur"


def uromanize(batch: Batch) -> Batch:
    """Uromanize text."""
    clean_string = re.sub(chars_to_remove_regex, "", batch["sentence"]).lower()
    batch["sentence"] = ur.romanize_string(clean_string, lcode=target_lang)
    return batch


common_voice_train = common_voice_train.map(uromanize)
common_voice_test = common_voice_test.map(uromanize)

Vocab = dict[str, Any]


def extract_all_chars(batch: Batch) -> Vocab:
    """Extract all chars from batch."""
    all_text = " ".join(batch["sentence"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}


sample_rate = 16_000

common_voice_train = common_voice_train.cast_column(
    "audio", Audio(sampling_rate=sample_rate)
)
common_voice_test = common_voice_test.cast_column(
    "audio", Audio(sampling_rate=sample_rate)
)

rand_int = random.randint(0, len(common_voice_train) - 1)

print("Target text:", common_voice_train[rand_int]["sentence"])
print("Input array shape:", common_voice_train[rand_int]["audio"]["array"].shape)
print("Sampling rate:", common_voice_train[rand_int]["audio"]["sampling_rate"])


hf_repo = "mms-meta/mms-zeroshot-300m"
print(f"hf_repo: {hf_repo}")
tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(hf_repo)
vocab_dict = tokenizer.vocab

new_vocab_dict = {target_lang: vocab_dict}

with open(".output/vocab.json", "w") as vocab_file:
    json.dump(new_vocab_dict, vocab_file)


tokenizer: Wav2Vec2CTCTokenizer = Wav2Vec2CTCTokenizer.from_pretrained(
    ".output/",
    unk_token=tokenizer.unk_token,
    pad_token=tokenizer.pad_token,
    word_delimiter_token=tokenizer.word_delimiter_token,
    bos_token=tokenizer.bos_token,
    eos_token=tokenizer.eos_token,
    target_lang=target_lang,
)

repo_name = "mms-300m-turkish"

tokenizer.push_to_hub(repo_name)  # type: ignore

feature_extractor = Wav2Vec2FeatureExtractor(
    feature_size=1,
    sampling_rate=sample_rate,
    padding_value=0.0,
    do_normalize=True,
    return_attention_mask=True,
)


@runtime_checkable
class HasCustomFields(Protocol):
    """Just for pyright type checking."""

    tokenizer: Wav2Vec2CTCTokenizer
    feature_extractor: Wav2Vec2FeatureExtractor


processor: Wav2Vec2Processor = Wav2Vec2Processor(
    feature_extractor=feature_extractor, tokenizer=tokenizer
)
assert isinstance(processor, HasCustomFields) and isinstance(
    processor, Wav2Vec2Processor
)


def prepare_dataset(batch: Batch) -> Batch:
    """Prepare dataset."""
    audio = batch["audio"]
    batch["input_values"] = processor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_values[0]
    batch["length"] = len(batch["input_values"])

    batch["labels"] = processor(text=batch["sentence"]).input_ids  # type: ignore
    return batch


common_voice_train = common_voice_train.map(
    prepare_dataset, remove_columns=common_voice_train.column_names
)
common_voice_test = common_voice_test.map(
    prepare_dataset, remove_columns=common_voice_test.column_names
)

access_key = os.getenv("HETZNER_ACCESS_KEY")
secret_key = os.getenv("HETZNER_SECRET_KEY")
endpoint = os.getenv("HETZNER_ENDPOINT")
region = os.getenv("HETZNER_REGION")


s3_client, s3_client_v2 = [
    boto3.client(
        "s3",
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        endpoint_url=endpoint,
        aws_session_token=None,
        config=boto3.session.Config(  # type: ignore
            retries={"max_attempts": 1, "mode": "standard"},
            signature_version=signature_version,
            region_name=region,
            s3=dict(addressing_style="virtual"),
        ),
    )
    for signature_version in ["s3v4", "s3"]
]

test_cache_path = f"./.app_cache/{data_seed}/test_set/"
train_cache_path = f"./.app_cache/{data_seed}/train_set/"
Path(test_cache_path).mkdir(parents=True, exist_ok=True)
Path(train_cache_path).mkdir(parents=True, exist_ok=True)

test_cache_bucket = f"{repo_name}-cache-{data_seed}-test-set"
train_cache_bucket = f"{repo_name}-cache-{data_seed}-train-set"

eval_size = 20_000
train_size = 100_000

eval_limit = 4_000

common_voice_test = common_voice_test.shuffle(seed=data_seed)
common_voice_train = common_voice_train.shuffle(seed=data_seed)

common_voice_test = ResizedDataset(common_voice_test, eval_size)
common_voice_train = ResizedDataset(common_voice_train, train_size)

orig_common_voice_test = common_voice_test
orig_common_voice_train = common_voice_train

common_voice_test = prepare_cached_dataset(
    common_voice_test,
    sample_rate,
    test_cache_path,
    test_cache_bucket,
    s3_client,
    s3_client_v2,
)
common_voice_train = prepare_cached_dataset(
    common_voice_train,
    sample_rate,
    train_cache_path,
    train_cache_bucket,
    s3_client,
    s3_client_v2,
)

common_voice_test = ResizedDataset(common_voice_test, eval_limit)

test_indices_order = list(range(len(common_voice_test)))
train_indices_order = list(range(len(common_voice_train)))

data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)

wer_metric = load("wer")
cer_metric = load("cer")


Metrics = dict[str, Any]


def compute_metrics(pred: EvalPrediction) -> Metrics:
    """Compute metrics."""
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    assert isinstance(processor, HasCustomFields)
    if isinstance(pred.label_ids, tuple):
        pred.label_ids = pred.label_ids[0]
    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id  # type: ignore

    pred_str = processor.batch_decode(pred_ids)
    # we do not want to group tokens when computing the metrics
    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    for i in range(100):
        print(f'({i}) pred: "{pred_str[i]}"')
        print(f'({i}) targ: "{label_str[i]}"')
    return {"wer": wer, "cer": cer}


model = CustomWav2Vec2ForCTC.from_pretrained(
    hf_repo,
    hidden_dropout=0.0,
    activation_dropout=0.0,
    attention_dropout=0.0,
    feat_proj_dropout=0.0,
    feat_quantizer_dropout=0.0,
    final_dropout=0.0,
    layerdrop=0.0,
    ctc_loss_reduction="mean",
    vocab_size=len(processor.tokenizer),
    adapter_attn_dim=len(processor.tokenizer),
    ignore_mismatched_sizes=True,
    # attn_implementation="flash_attention_2",
    # attn_implementation="sdpa",
)

model.init_adapter_layers()

model.freeze_base_model()
model.freeze_feature_encoder()

adapter_weights = model._get_adapters()
for param in adapter_weights.values():
    param.requires_grad = True

comet_ml.login(project_name=os.getenv("WANDB_PROJECT"))
wandb.init(dir="./.wandb")

num_train_epochs = 1
effective_batch_size = 16
per_device_train_batch_size = 4
per_device_eval_batch_size = 8
num_devices = 1
warmup_ratio = 0.0
decay_ratio = 1.0
learning_rate = 1e-2

global_batch_size = per_device_train_batch_size * num_devices
accumulation_steps = effective_batch_size // global_batch_size

num_training_steps = (
    len(common_voice_train) // effective_batch_size  # type: ignore
) * num_train_epochs

lr_scheduler_kwargs = dict(num_decay_steps=int(decay_ratio * num_training_steps))

training_args = CustomTrainingArguments(
    seed=seed,
    data_seed=data_seed,
    report_to=["comet_ml", "wandb"],
    output_dir=f".output/{repo_name}",
    run_name=repo_name,
    group_by_length=True,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_eval_batch_size,
    gradient_accumulation_steps=accumulation_steps,
    eval_strategy="steps",
    num_train_epochs=num_train_epochs,
    mega_batch_mult=100,
    dataloader_num_workers=4,
    # dataloader_num_workers=0,
    dataloader_drop_last=True,
    gradient_checkpointing=True,
    fp16=False,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=100,
    eval_on_start=True,
    logging_first_step=True,
    learning_rate=learning_rate,
    lr_scheduler_type="warmup_stable_decay",
    warmup_steps=int(warmup_ratio * num_training_steps),
    lr_scheduler_kwargs=lr_scheduler_kwargs,
    weight_decay=0.01,
    save_total_limit=2,
    # push_to_hub=True,
    push_to_hub=False,
    logging_nan_inf_filter=True,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    log_level="info",
    # torch_compile=True,
    torch_compile=False,
)

trainer = CustomTrainer(
    model=model,  # type: ignore
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    eval_dataset=common_voice_test,
    train_dataset=common_voice_train,
    processing_class=processor.feature_extractor,
    test_indices_order=test_indices_order,
    train_indices_order=train_indices_order,
    test_grouped_indices=common_voice_test.sync_safe_groups,
    train_grouped_indices=common_voice_train.sync_safe_groups,
)

trainer.train()
trainer.evaluate()

wandb.finish()
comet_ml.end()

adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(target_lang)
assert training_args.output_dir is not None
adapter_file = os.path.join(training_args.output_dir, adapter_file)

safe_save_file(model._get_adapters(), adapter_file, metadata={"format": "pt"})

trainer.push_to_hub()

model_id = f"Kellner/{repo_name}"

model = CustomWav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang).to(
    "cuda"  # type: ignore
)
_processor = Wav2Vec2Processor.from_pretrained(model_id)
assert isinstance(_processor, HasCustomFields) and isinstance(
    _processor, Wav2Vec2Processor
)
processor = _processor

processor.tokenizer.set_target_lang(target_lang)  # type: ignore

common_voice_test_tr: HFDataset = load_dataset(
    "mozilla-foundation/common_voice_17_0",
    "tr",
    data_dir="./cv-corpus-17.0-2024-03-20",
    split="test",
    token=True,
    trust_remote_code=True,
    # streaming=True,
)  # type: ignore
common_voice_test_tr = common_voice_test_tr.cast_column(
    "audio", Audio(sampling_rate=sample_rate)
)

input_dict = processor(
    common_voice_test_tr[0]["audio"]["array"],
    sampling_rate=sample_rate,
    return_tensors="pt",
    padding=True,
)

logits = model(input_dict.input_values.to("cuda")).logits

pred_ids = torch.argmax(logits, dim=-1)[0]

print("Prediction:")
print(processor.decode(pred_ids))

print("\nReference:")
print(common_voice_test_tr[0]["sentence"].lower())
