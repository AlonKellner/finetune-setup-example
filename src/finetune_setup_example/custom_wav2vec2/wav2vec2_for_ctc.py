"""Custom wav2vec2 for CTC implementation.

Most notably, uses the flat labels as generated by the CTC collator, much faster than the original version.
"""

import torch
from torch import nn
from transformers import Wav2Vec2Config, Wav2Vec2ForCTC, Wav2Vec2Model
from transformers.models.wav2vec2.modeling_wav2vec2 import (
    _HIDDEN_STATES_START_POSITION,
    CausalLMOutput,
)


class CustomWav2Vec2ForCTC(Wav2Vec2ForCTC):
    """Custom faster wav2vec2 implementation."""

    def __init__(self, config: Wav2Vec2Config, target_lang: str | None = None) -> None:
        super().__init__(config)

        self.wav2vec2 = Wav2Vec2Model(config)
        self.dropout = nn.Dropout(config.final_dropout)

        self.target_lang = target_lang

        if config.vocab_size is None:
            raise ValueError(
                f"You are trying to instantiate {self.__class__} with a configuration that "
                "does not define the vocabulary size of the language model head. Please "
                "instantiate the model as follows: `Wav2Vec2ForCTC.from_pretrained(..., vocab_size=vocab_size)`. "
                "or define `vocab_size` of your model's configuration."
            )
        output_hidden_size = (
            config.output_hidden_size
            if hasattr(config, "add_adapter") and config.add_adapter
            else config.hidden_size
        )
        self.lm_head = nn.Linear(output_hidden_size, config.vocab_size)

        # Initialize weights and apply final processing
        self.post_init()

    def forward(
        self,
        input_values: torch.Tensor | None,
        attention_mask: torch.Tensor | None = None,
        output_attentions: bool | None = None,
        output_hidden_states: bool | None = None,
        return_dict: bool | None = None,
        labels: torch.Tensor | None = None,
        flat_labels: torch.Tensor | None = None,
    ) -> tuple | CausalLMOutput:
        """Forward."""
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        if labels is not None and labels.max() >= self.config.vocab_size:
            raise ValueError(
                f"Label values must be <= vocab_size: {self.config.vocab_size}"
            )

        outputs = self.wav2vec2(
            input_values,
            attention_mask=attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = outputs[0]
        hidden_states = self.dropout(hidden_states)

        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None and flat_labels is not None:
            # retrieve loss input_lengths from attention_mask
            attention_mask = (
                attention_mask
                if attention_mask is not None
                else torch.ones_like(input_values, dtype=torch.long)  # type: ignore
            )  # type: ignore
            input_lengths = self._get_feat_extract_output_lengths(
                attention_mask.sum(-1)  # type: ignore
            ).to(torch.long)  # type: ignore

            # assuming that padded tokens are filled with -100
            # when not being attended to
            target_lengths, flattened_targets = self.labels_for_ctc(labels, flat_labels)

            # ctc_loss doesn't support fp16
            log_probs = nn.functional.log_softmax(
                logits, dim=-1, dtype=torch.float32
            ).transpose(0, 1)

            with torch.backends.cudnn.flags(enabled=True, benchmark=True):
                loss = nn.functional.ctc_loss(
                    log_probs,
                    flattened_targets,
                    input_lengths,
                    target_lengths,
                    blank=self.config.pad_token_id,
                    reduction=self.config.ctc_loss_reduction,
                    zero_infinity=self.config.ctc_zero_infinity,
                )

        if not return_dict:
            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]
            return (loss, *output) if loss is not None else output

        return CausalLMOutput(
            loss=loss,  # type: ignore
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

    def labels_for_ctc(
        self, labels: torch.Tensor, flat_labels: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Return labels formatting for CTC loss."""
        labels_mask = labels >= 0
        target_lengths = labels_mask.sum(-1)
        return target_lengths, flat_labels
